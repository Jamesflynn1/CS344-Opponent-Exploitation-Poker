# Copyright 2019 DeepMind Technologies Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Python implementation of the counterfactual regret minimization algorithm.

One iteration of CFR consists of:
1) Compute current strategy from regrets (e.g. using Regret Matching).
2) Compute values using the current strategy
3) Compute regrets from these values

The average policy is what converges to a Nash Equilibrium.
"""

import collections
import attr
import copy
import numpy as np
from scipy import optimize
from scipy import linalg
from collections import defaultdict

from open_spiel.python import policy
#from 
from scipy.linalg import lstsq
import pyspiel
import Deviation_Types.Deviation
import Deviation_Types.Swap_Transform




@attr.s
class _InfoStateNode(object):
  """An object wrapping values associated to an information state."""
  # The list of the legal actions.
  legal_actions = attr.ib()
  index_in_tabular_policy = attr.ib()
  # Map from information states string representations and actions to the
  # counterfactual regrets, accumulated over the policy iterations


  # The newly availible deviations + the old ones
  relizable_deviations = attr.ib()
  #deviations_transform_group = attr.ib()
  #Formed from current strategy at regret matching

  #Player -> state -> action -> prob
  current_history_probs = attr.ib()

  #An array representing
  history = attr.ib()
  updates = attr.ib()
  updated = attr.ib()

  cumulative_regret = attr.ib(factory=lambda: collections.defaultdict(float))
  # Same as above for the cumulative of the policy probabilities computed
  # during the policy iterations
  cumulative_policy = attr.ib(factory=lambda: collections.defaultdict(float))
  y_values = attr.ib(factory=lambda: collections.defaultdict(float))


class _EFRSolverBase(object):
  def __init__(self, game, alternating_updates, linear_averaging,
               regret_matching_plus, _deviation_gen):
    # pyformat: disable
    """Initializer.

    Args:
      game: The `pyspiel.Game` to run on.
      alternating_updates: If `True`, alternating updates are performed: for
        each player, we compute and update the cumulative regrets and policies.
        In that case, and when the policy is frozen during tree traversal, the
        cache is reset after each update for one player.
        Otherwise, the update is simultaneous.
      linear_averaging: Whether to use linear averaging, i.e.
        cumulative_policy[info_state][action] += (
          iteration_number * reach_prob * action_prob)

        or not:

        cumulative_policy[info_state][action] += reach_prob * action_prob
      regret_matching_plus: Whether to use Regret Matching+:
        cumulative_regrets = max(cumulative_regrets + regrets, 0)
        or simply regret matching:
        cumulative_regrets = cumulative_regrets + regrets
    """
    # pyformat: enable
    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, (
        "CFR requires sequential games. If you're trying to run it " +
        "on a simultaneous (or normal-form) game, please first transform it " +
        "using turn_based_simultaneous_game.")

    self._game = game
    self._num_players = game.num_players()
    self._root_node = self._game.new_initial_state()

    # This is for returning the current policy and average policy to a caller
    self._current_policy = policy.TabularPolicy(game)
    self._average_policy = self._current_policy.__copy__()
    self._deviation_gen = _deviation_gen

    self._info_state_nodes = {}
    hist = {player : [] for player in range(self._num_players)}
    #hist = dict.fromkeys(range(self._num_players), list())
    self._initialize_info_state_nodes(self._root_node, hist, [[] for _ in range(self._num_players)],[[] for _ in range(self._num_players)])

    self._iteration = 1  # For possible linear-averaging.
    self._linear_averaging = linear_averaging
    self._alternating_updates = alternating_updates
    self._regret_matching_plus = regret_matching_plus
    self._str_to_action = {}
  def return_cumulative_regret(self):
    return {list(self._info_state_nodes.keys())[i]: list(self._info_state_nodes.values())[i].cumulative_regret for i in range(len(self._info_state_nodes.keys()))}
  def current_policy(self):
    return self._current_policy

  def average_policy(self):
    _update_average_policy(self._average_policy, self._info_state_nodes)
    return self._average_policy

  def _initialize_info_state_nodes(self, state, history, uniform_probs_to_state,path_indices):
    """Initializes info_state_nodes.

    Create one _InfoStateNode per infoset. We could also initialize the node
    when we try to access it and it does not exist.

    Args:
      state: The current state in the tree walk. This should be the root node
        when we call this function from a CFR solver.
    """
    if state.is_terminal():
      return

    if state.is_chance_node():
      for action, unused_action_prob in state.chance_outcomes():
        self._initialize_info_state_nodes(state.child(action), history, uniform_probs_to_state, path_indices)
      return
    
    current_player = state.current_player()
    info_state = state.information_state_string(current_player)
    info_state_node = self._info_state_nodes.get(info_state)
    if info_state_node is None:
      legal_actions = state.legal_actions(current_player)
      info_state_node = _InfoStateNode(
          legal_actions=legal_actions,
          index_in_tabular_policy=self._current_policy.state_lookup[info_state],
          relizable_deviations = None,
          history = history[current_player].copy(),
          current_history_probs = copy.deepcopy(path_indices[current_player]),
          updates = 0,
          updated = False
          )

      info_state_node.relizable_deviations = self._deviation_gen(len(info_state_node.legal_actions), info_state_node.history)
      self._info_state_nodes[info_state] = info_state_node

    legal_actions = state.legal_actions(current_player)
    new_uniform_probs_to_state = copy.deepcopy(uniform_probs_to_state)
    assert len(new_uniform_probs_to_state[current_player]) == len(history[current_player])

    new_uniform_probs_to_state[current_player].append({legal_actions[i]: 1/len(legal_actions) for i in range(len(legal_actions))})
    for action in info_state_node.legal_actions:
      new_path_indices = copy.deepcopy(path_indices)
      new_path_indices[current_player].append([legal_actions, info_state_node.index_in_tabular_policy])
      new_history = copy.deepcopy(history)
      new_history[current_player].append(action)
      assert len(new_history[current_player]) == len(new_path_indices[current_player])
      
      self._initialize_info_state_nodes(state.child(action), new_history, new_uniform_probs_to_state, new_path_indices)

  def _update_current_policy(self,state, current_policy):
    """Updated in order so that memory reach probs are defined wrt to the new strategy
    """

    if state.is_terminal():
      return 
    elif not state.is_chance_node():
      current_player = state.current_player()
      info_state = state.information_state_string(current_player)
      info_state_node = self._info_state_nodes[info_state]
      deviations = info_state_node.relizable_deviations
      #print(info_state)
      for devation in range(len(deviations)):
        #change too infostate
        mem_reach_probs = create_probs_from_index(info_state_node.current_history_probs, current_policy)
        deviation_reach_prob = deviations[devation].player_deviation_reach_probability(mem_reach_probs)
        info_state_node.y_values[deviations[devation]] += max(0,info_state_node.cumulative_regret[devation])*deviation_reach_prob

      #Might be incorrect
      state_policy = current_policy.policy_for_key(info_state)
      state_cum_policy = info_state_node.cumulative_policy
      cum_policy_sum = sum(info_state_node.cumulative_policy.values())
      #print 
      for action, value in self._regret_matching(info_state_node.legal_actions, info_state_node, info_state).items():
        state_policy[action] = value
      info_state_node.updated = True

      info_state_node.updates +=1 

      for action in info_state_node.legal_actions:
        new_state = state.child(action)
        self._update_current_policy(new_state, current_policy)
    else:
      for action, action_prob in state.chance_outcomes():
        new_state = state.child(action)
        self._update_current_policy(new_state, current_policy)
  #Path to state probability ignores chance probabilty as this is stored as new_reach_probabilities[-1]
  def _compute_cumulative_immediate_regret_for_player(self, state, policies,
                                                reach_probabilities, player):
    #Reset the y values from the previous iteration
    if state.is_terminal():
      return np.asarray(state.returns())

    if state.is_chance_node():
      state_value = 0.0
      for action, action_prob in state.chance_outcomes():
        assert action_prob > 0
        new_state = state.child(action)
        new_reach_probabilities = reach_probabilities.copy()
        new_reach_probabilities[-1] *= action_prob

        state_value += action_prob * self._compute_cumulative_immediate_regret_for_player(
            new_state, policies, new_reach_probabilities, player)
      return state_value

    current_player = state.current_player()
    info_state = state.information_state_string(current_player)



    # No need to continue on this history branch as no update will be performed
    # for any player.
    # The value we return here is not used in practice. If the conditional
    # statement is True, then the last taken action has probability 0 of
    # occurring, so the returned value is not impacting the parent node value.
    if all(reach_probabilities[:-1] == 0):
      return np.zeros(self._num_players)

    state_value = np.zeros(self._num_players)

    # The utilities of the children states are computed recursively. As the
    # regrets are added to the information state regrets for each state in that
    # information state, the recursive call can only be made once per child
    # state. Therefore, the utilities are cached.
    children_utilities = {}

    info_state_node = self._info_state_nodes[info_state]
    #Reset y values
    info_state_node.y_values = collections.defaultdict(float)
    if policies is None:
      info_state_policy = self._get_infostate_policy(info_state)
    else:
      info_state_policy = policies[current_player](info_state)

    reach_prob = reach_probabilities[current_player]
    for action in state.legal_actions():
      action_prob = info_state_policy.get(action, 0.)
      info_state_node.cumulative_policy[action] += action_prob * reach_prob
      new_state = state.child(action)
      new_reach_probabilities = reach_probabilities.copy()
      assert action_prob <= 1
      new_reach_probabilities[current_player] *= action_prob
      child_utility = self._compute_cumulative_immediate_regret_for_player(new_state,policies=policies,reach_probabilities=new_reach_probabilities,player=player)

      state_value += action_prob * child_utility
      children_utilities[action] = child_utility

    counterfactual_reach_prob = (np.prod(reach_probabilities[:current_player]) * np.prod(reach_probabilities[current_player + 1:]))

    state_value_for_player = state_value[current_player]
    deviations = info_state_node.relizable_deviations
    #print("pass")
    for deviationIndex in range(len(deviations)):
      #FIX ADD DICT TO ARRAY CONVERSION FUNCTION
      deviation = deviations[deviationIndex]
      deviation_strategy = deviation.deviate(strat_dict_to_array(self._get_infostate_policy(info_state)))

      player_child_utilities = np.array(list(children_utilities.values()))[:,current_player]
      devation_cf_value =  np.inner(np.transpose(deviation_strategy), player_child_utilities)

      memory_reach_probs = create_probs_from_index(info_state_node.current_history_probs,self.current_policy())
      player_current_memory_reach_prob = deviation.player_deviation_reach_probability(memory_reach_probs)
      
      deviation_regret = player_current_memory_reach_prob  *((devation_cf_value*counterfactual_reach_prob) - (counterfactual_reach_prob * state_value_for_player))

      info_state_node.cumulative_regret[deviationIndex] += deviation_regret
    return state_value


  def _get_infostate_policy(self, info_state_str):
    """Returns an {action: prob} dictionary for the policy on `info_state`."""
    info_state_node = self._info_state_nodes[info_state_str]
    prob_vec = self._current_policy.action_probability_array[
        info_state_node.index_in_tabular_policy]
    return {
        action: prob_vec[action] for action in info_state_node.legal_actions
    }
def __get_infostate_policy_array(self, info_state_str):
    info_state_node = self._info_state_nodes[info_state_str]
    return self._current_policy.action_probability_array[
    info_state_node.index_in_tabular_policy]

class _EFRSolver(_EFRSolverBase):
  def __init__(self, game, alternating_updates, linear_averaging, regret_matching_plus, _deviation_gen):
    super().__init__(game, alternating_updates, linear_averaging, regret_matching_plus, _deviation_gen)

  def evaluate_and_update_policy(self):
    """Performs a single step of policy evaluation and policy improvement."""
    self._compute_cumulative_immediate_regret_for_player(
      self._root_node,
      policies=None,
      reach_probabilities=np.ones(self._game.num_players() + 1),
      player=None)
    history = [ [] for _ in range(self._num_players)]
    #NONE IS NOT A GENERALISED FIX, REPLACE
    self._update_current_policy(self._root_node,self._current_policy)
    self._iteration+= 1

class EFRSolver(_EFRSolver):
  def __init__(self, game, deviation_sets, external_only):
    super(EFRSolver, self).__init__(
        game, 
        regret_matching_plus=False,
        alternating_updates=False,
        linear_averaging=False,
        _deviation_gen = deviation_sets,
        )
    self._external_only = external_only
  def _regret_matching(self, legal_actions, info_set_node,info_state):
    """Returns an info state policy by applying regret-matching.


    Args:
      cumulative_regrets: A {deviation: y value} dictionary.
      legal_actions: the list of legal actions at this state.

    Returns:
      A dict of action -> prob for all legal actions.
    """
    z = sum(info_set_node.y_values.values())
    info_state_policy = {}

    if self._external_only and z > 0:
      weighted_deviation_matrix = np.zeros((len(legal_actions), len(legal_actions)))
      for dev in list(info_set_node.y_values.keys()): 
        weighted_deviation_matrix += (info_set_node.y_values[dev]/z) * dev.return_transform_matrix()
      new_strategy = weighted_deviation_matrix[:,0]/ sum(weighted_deviation_matrix[:,0])
      for index in range(len(legal_actions)):
        info_state_policy[legal_actions[index]] = new_strategy[index]
      return info_state_policy

    elif z > 1e-14:
      #Use previous policy
      num_actions = len(info_set_node.legal_actions)
      weighted_deviation_matrix = -np.eye(num_actions)
      for dev in list(info_set_node.y_values.keys()): 
        weighted_deviation_matrix += (info_set_node.y_values[dev]/z) * dev.return_transform_matrix()
      new_strategy = np.full(len(info_set_node.legal_actions), 1.0 / len(info_set_node.legal_actions))

      last_row = np.ones(num_actions)
      weighted_deviation_matrix = np.vstack([weighted_deviation_matrix, last_row])
      b = np.zeros(num_actions+1)
      b[num_actions] = 1
      b = np.reshape(b, (num_actions+1, 1))

      strategy = lstsq(weighted_deviation_matrix, b)[0]
      normalised_strategy = strategy
      normalised_strategy[np.where(normalised_strategy<0)] = 0
      normalised_strategy[np.where(normalised_strategy>1)] = 1

      normalised_strategy = normalised_strategy/sum(normalised_strategy)
      
      new_strategy = normalised_strategy
      #for i in range(len(new_strategy)):
      #  value =  (sum(weighted_deviation_matrix[i,:]) - sum(weighted_deviation_matrix[:i,i]) - sum(weighted_deviation_matrix[i+1:,i]))
      #  new_strategy[i] = value
      #mv = abs(np.min(new_strategy))
      #new_strategy = new_strategy+mv
      #new_strategy = new_strategy / sum(new_strategy)
      #for i in range()
      #new_strategy = weighted_deviation_matrix[:,0]/ sum(weighted_deviation_matrix[:,0])

      for index in range(len(new_strategy)):
        info_state_policy[info_set_node.legal_actions[index]] = max(new_strategy[index],0)
    else:
      for index in range(len(legal_actions)):
        info_state_policy[legal_actions[index]] = 1.0 / len(legal_actions)
    return info_state_policy
        
def _update_average_policy(average_policy, info_state_nodes):
  """Updates in place `average_policy` to the average of all policies iterated.

  This function is a module level function to be reused by both CFRSolver and
  CFRBRSolver.

  Args:
    average_policy: A `policy.TabularPolicy` to be updated in-place.
    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.
  """
  for info_state, info_state_node in info_state_nodes.items():
    info_state_policies_sum = info_state_node.cumulative_policy
    state_policy = average_policy.policy_for_key(info_state)
    probabilities_sum = sum(info_state_policies_sum.values())
    if probabilities_sum == 0:
      num_actions = len(info_state_node.legal_actions)
      for action in info_state_node.legal_actions:
        state_policy[action] = 1 / num_actions
    else:
      for action, action_prob_sum in info_state_policies_sum.items():
        state_policy[action] = action_prob_sum / probabilities_sum
def strat_dict_to_array(sd):
  actions = list(sd.keys())
  strategy = np.zeros((len(actions),1))
  for action in range(len(actions)):
    strategy[action][0] = sd[actions[action]]
  return strategy

def array_to_strat_dict(sa, legal_actions):
  sd = {}
  for action in legal_actions:
    sd[action] = sa[action]
  return sd

def reset_update_var(info_nodes):
  for node in list(info_nodes.values()):
    node.updated = False
  
def create_probs_from_index(indices, current_policy):
  path_to_state = []
  if indices == None or len(indices) == 0:
    return []
  for index in indices:
    strat_dict = array_to_strat_dict(current_policy.action_probability_array[index[1]], index[0])
    path_to_state.append(strat_dict)
  return path_to_state