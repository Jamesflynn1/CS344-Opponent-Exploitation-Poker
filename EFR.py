# Copyright 2019 DeepMind Technologies Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Python implementation of the counterfactual regret minimization algorithm.

One iteration of CFR consists of:
1) Compute current strategy from regrets (e.g. using Regret Matching).
2) Compute values using the current strategy
3) Compute regrets from these values

The average policy is what converges to a Nash Equilibrium.
"""

import collections
import attr
import copy
import numpy as np
from collections import defaultdict

from open_spiel.python import policy
#from 
from scipy.linalg import lstsq
import pyspiel
import Deviation_Types.Deviation
import Deviation_Types.Swap_Transform
from Deviation_Types.Deviation_Sets import *




@attr.s
class _InfoStateNode(object):
  """An object wrapping values associated to an information state."""
  # The list of the legal actions.
  legal_actions = attr.ib()
  index_in_tabular_policy = attr.ib()
  # The newly availible deviations + the old ones
  relizable_deviations = attr.ib()
  #Player -> state -> action -> prob
  current_history_probs = attr.ib()

  #An array representing
  history = attr.ib()
  updates = attr.ib()
  updated = attr.ib()

  cumulative_regret = attr.ib(factory=lambda: collections.defaultdict(float))
  # Same as above for the cumulative of the policy probabilities computed
  # during the policy iterations
  cumulative_policy = attr.ib(factory=lambda: collections.defaultdict(float))
  y_values = attr.ib(factory=lambda: collections.defaultdict(float))


class _EFRSolverBase(object):
  def __init__(self, game, alternating_updates, linear_averaging,
               regret_matching_plus, _deviation_gen):
    # pyformat: disable
    """Initializer.

    Args:
      game: The `pyspiel.Game` to run on.
      alternating_updates: If `True`, alternating updates are performed: for
        each player, we compute and update the cumulative regrets and policies.
        In that case, and when the policy is frozen during tree traversal, the
        cache is reset after each update for one player.
        Otherwise, the update is simultaneous.
      linear_averaging: Whether to use linear averaging, i.e.
        cumulative_policy[info_state][action] += (
          iteration_number * reach_prob * action_prob)

        or not:

        cumulative_policy[info_state][action] += reach_prob * action_prob
      regret_matching_plus: Whether to use Regret Matching+:
        cumulative_regrets = max(cumulative_regrets + regrets, 0)
        or simply regret matching:
        cumulative_regrets = cumulative_regrets + regrets
    """
    # pyformat: enable
    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, (
        "CFR requires sequential games. If you're trying to run it " +
        "on a simultaneous (or normal-form) game, please first transform it " +
        "using turn_based_simultaneous_game.")

    self._game = game
    self._num_players = game.num_players()
    self._root_node = self._game.new_initial_state()

    # This is for returning the current policy and average policy to a caller
    self._current_policy = policy.TabularPolicy(game)
    self._average_policy = self._current_policy.__copy__()
    self._deviation_gen = _deviation_gen

    self._info_state_nodes = {}
    hist = {player : [] for player in range(self._num_players)}
    #hist = dict.fromkeys(range(self._num_players), list())
    self._initialize_info_state_nodes(self._root_node, hist, [[] for _ in range(self._num_players)],[[] for _ in range(self._num_players)])

    self._iteration = 1  # For possible linear-averaging.
    self._linear_averaging = linear_averaging
    self._alternating_updates = alternating_updates
    self._regret_matching_plus = regret_matching_plus
    self._str_to_action = {}
  def return_cumulative_regret(self):
    return {list(self._info_state_nodes.keys())[i]: list(self._info_state_nodes.values())[i].cumulative_regret for i in range(len(self._info_state_nodes.keys()))}
  def current_policy(self):
    return self._current_policy

  def average_policy(self):
    _update_average_policy(self._average_policy, self._info_state_nodes)
    return self._average_policy

  def _initialize_info_state_nodes(self, state, history, uniform_probs_to_state,path_indices):
    """Initializes info_state_nodes.

    Create one _InfoStateNode per infoset. We could also initialize the node
    when we try to access it and it does not exist.

    Args:
      state: The current state in the tree walk. This should be the root node
        when we call this function from a CFR solver.
    """
    if state.is_terminal():
      return

    if state.is_chance_node():
      for action, unused_action_prob in state.chance_outcomes():
        self._initialize_info_state_nodes(state.child(action), history, uniform_probs_to_state, path_indices)
      return
    
    current_player = state.current_player()
    info_state = state.information_state_string(current_player)
    info_state_node = self._info_state_nodes.get(info_state)
    if info_state_node is None:
      legal_actions = state.legal_actions(current_player)
      info_state_node = _InfoStateNode(
          legal_actions=legal_actions,
          index_in_tabular_policy=self._current_policy.state_lookup[info_state],
          relizable_deviations = None,
          history = history[current_player].copy(),
          current_history_probs = copy.deepcopy(path_indices[current_player]),
          updates = 0,
          updated = False
          )
      prior_possible_actions = []
      for i in range(len(info_state_node.current_history_probs)):
        prior_possible_actions.append(info_state_node.current_history_probs[i][0])
      prior_possible_actions.append(info_state_node.legal_actions)

      info_state_node.relizable_deviations = self._deviation_gen(len(info_state_node.legal_actions), info_state_node.history, prior_possible_actions)
      self._info_state_nodes[info_state] = info_state_node

    legal_actions = state.legal_actions(current_player)
    new_uniform_probs_to_state = copy.deepcopy(uniform_probs_to_state)
    assert len(new_uniform_probs_to_state[current_player]) == len(history[current_player])

    new_uniform_probs_to_state[current_player].append({legal_actions[i]: 1/len(legal_actions) for i in range(len(legal_actions))})
    for action in info_state_node.legal_actions:
      #Speedup
      new_path_indices = copy.deepcopy(path_indices)
      new_path_indices[current_player].append([legal_actions, info_state_node.index_in_tabular_policy])
      #Speedup
      new_history = copy.deepcopy(history)
      new_history[current_player].append(action)
      assert len(new_history[current_player]) == len(new_path_indices[current_player])
      
      self._initialize_info_state_nodes(state.child(action), new_history, new_uniform_probs_to_state, new_path_indices)

  def _update_current_policy(self,state, current_policy):
    """Updated in order so that memory reach probs are defined wrt to the new strategy
    """

    if state.is_terminal():
      return 
    elif not state.is_chance_node():
      current_player = state.current_player()
      info_state = state.information_state_string(current_player)
      info_state_node = self._info_state_nodes[info_state]
      deviations = info_state_node.relizable_deviations
      #print(info_state)
      for devation in range(len(deviations)):
        #change too infostate
        mem_reach_probs = create_probs_from_index(info_state_node.current_history_probs, current_policy)
        deviation_reach_prob = deviations[devation].player_deviation_reach_probability(mem_reach_probs)
        info_state_node.y_values[deviations[devation]] += max(0,info_state_node.cumulative_regret[devation])*deviation_reach_prob

      #Might be incorrect
      state_policy = current_policy.policy_for_key(info_state)
      #print 
      for action, value in self._regret_matching(info_state_node.legal_actions, info_state_node).items():
        state_policy[action] = value
      info_state_node.updated = True

      info_state_node.updates +=1 

      for action in info_state_node.legal_actions:
        new_state = state.child(action)
        self._update_current_policy(new_state, current_policy)
    else:
      for action, action_prob in state.chance_outcomes():
        new_state = state.child(action)
        self._update_current_policy(new_state, current_policy)
  #Path to state probability ignores chance probabilty as this is stored as new_reach_probabilities[-1]
  def _compute_cumulative_immediate_regret_for_player(self, state, policies,
                                                reach_probabilities, player):
    #Reset the y values from the previous iteration
    if state.is_terminal():
      return np.asarray(state.returns())

    if state.is_chance_node():
      state_value = 0.0
      for action, action_prob in state.chance_outcomes():
        assert action_prob > 0
        new_state = state.child(action)
        new_reach_probabilities = reach_probabilities.copy()
        new_reach_probabilities[-1] *= action_prob

        state_value += action_prob * self._compute_cumulative_immediate_regret_for_player(
            new_state, policies, new_reach_probabilities, player)
      return state_value

    current_player = state.current_player()
    info_state = state.information_state_string(current_player)



    # No need to continue on this history branch as no update will be performed
    # for any player.
    # The value we return here is not used in practice. If the conditional
    # statement is True, then the last taken action has probability 0 of
    # occurring, so the returned value is not impacting the parent node value.
    if all(reach_probabilities[:-1] == 0):
      return np.zeros(self._num_players)

    state_value = np.zeros(self._num_players)

    # The utilities of the children states are computed recursively. As the
    # regrets are added to the information state regrets for each state in that
    # information state, the recursive call can only be made once per child
    # state. Therefore, the utilities are cached.
    children_utilities = {}

    info_state_node = self._info_state_nodes[info_state]
    #Reset y values
    info_state_node.y_values = collections.defaultdict(float)
    if policies is None:
      info_state_policy = self._get_infostate_policy(info_state)
    else:
      info_state_policy = policies[current_player](info_state)

    reach_prob = reach_probabilities[current_player]
    for action in state.legal_actions():
      action_prob = info_state_policy.get(action, 0.)
      info_state_node.cumulative_policy[action] += action_prob * reach_prob
      new_state = state.child(action)
      new_reach_probabilities = reach_probabilities.copy()
      assert action_prob <= 1
      new_reach_probabilities[current_player] *= action_prob
      child_utility = self._compute_cumulative_immediate_regret_for_player(new_state,policies=policies,reach_probabilities=new_reach_probabilities,player=player)

      state_value += action_prob * child_utility
      children_utilities[action] = child_utility

    counterfactual_reach_prob = (np.prod(reach_probabilities[:current_player]) * np.prod(reach_probabilities[current_player + 1:]))

    state_value_for_player = state_value[current_player]
    deviations = info_state_node.relizable_deviations
    for deviationIndex in range(len(deviations)):
      #FIX ADD DICT TO ARRAY CONVERSION FUNCTION
      deviation = deviations[deviationIndex]
      deviation_strategy = deviation.deviate(strat_dict_to_array(self._get_infostate_policy(info_state)))

      player_child_utilities = np.array(list(children_utilities.values()))[:,current_player]
      devation_cf_value =  np.inner(np.transpose(deviation_strategy), player_child_utilities)

      memory_reach_probs = create_probs_from_index(info_state_node.current_history_probs,self.current_policy())
      player_current_memory_reach_prob = deviation.player_deviation_reach_probability(memory_reach_probs)
      
      deviation_regret = player_current_memory_reach_prob  *((devation_cf_value*counterfactual_reach_prob) - (counterfactual_reach_prob * state_value_for_player))

      info_state_node.cumulative_regret[deviationIndex] += deviation_regret
    return state_value


  def _get_infostate_policy(self, info_state_str):
    """Returns an {action: prob} dictionary for the policy on `info_state`."""
    info_state_node = self._info_state_nodes[info_state_str]
    prob_vec = self._current_policy.action_probability_array[
        info_state_node.index_in_tabular_policy]
    return {
        action: prob_vec[action] for action in info_state_node.legal_actions
    }
def __get_infostate_policy_array(self, info_state_str):
    info_state_node = self._info_state_nodes[info_state_str]
    return self._current_policy.action_probability_array[
    info_state_node.index_in_tabular_policy]

class _EFRSolver(_EFRSolverBase):
  def __init__(self, game, alternating_updates, linear_averaging, regret_matching_plus, _deviation_gen):
    super().__init__(game, alternating_updates, linear_averaging, regret_matching_plus, _deviation_gen)

  def evaluate_and_update_policy(self):
    """Performs a single step of policy evaluation and policy improvement."""
    self._compute_cumulative_immediate_regret_for_player(
      self._root_node,
      policies=None,
      reach_probabilities=np.ones(self._game.num_players() + 1),
      player=None)
    history = [ [] for _ in range(self._num_players)]
    #NONE IS NOT A GENERALISED FIX, REPLACE
    self._update_current_policy(self._root_node,self._current_policy)
    self._iteration+= 1

class EFRSolver(_EFRSolver):
  def __init__(self, game, deviations_name):

    #Takes the deviation sets used for learning from Deviation_Sets
    external_only = False
    deviation_sets = None

    if deviations_name == "blind action":
      deviation_sets = return_blind_action
      external_only = True
    elif deviations_name == "informed action":
      deviation_sets = return_informed_action
    elif deviations_name == "blind cf" or deviations_name == "blind counterfactual":
      deviation_sets = return_blind_CF
      external_only = True
    elif deviations_name == "informed cf" or deviations_name == "informed counterfactual":
      deviation_sets = return_informed_CF
    elif deviations_name == "bps" or deviations_name == "blind partial sequence":
      deviation_sets = return_blind_partial_sequence
      external_only = True
    elif deviations_name == "cfps" or deviations_name == "cf partial sequence" or deviations_name == "counterfactual partial sequence":
      deviation_sets = return_cf_partial_sequence
    elif deviations_name == "csps" or deviations_name == "casual partial sequence":
      deviation_sets = return_cs_partial_sequence
    elif deviations_name == "tips" or "twice informed partial sequence":
      deviation_sets = return_twice_informed_partial_sequence
    elif deviations_name == "bhv" or deviations_name == "single target behavioural" or "behavioural":
      deviation_sets = return_behavourial
    else:
      print("Unsupported Deviation Set")
      return None
    super(EFRSolver, self).__init__(game, 
        regret_matching_plus=False,
        alternating_updates=False,
        linear_averaging=False,
        _deviation_gen = deviation_sets,
        )
    self._external_only = external_only
  def _regret_matching(self, legal_actions, info_set_node):
    """Returns an info state policy by applying regret-matching.


    Args:
      cumulative_regrets: A {deviation: y value} dictionary.
      legal_actions: the list of legal actions at this state.

    Returns:
      A dict of action -> prob for all legal actions.
    """
    z = sum(info_set_node.y_values.values())
    info_state_policy = {}

    #The fixed point solution can be directly obtained through the weighted regret matrix if only external deviations are used
    if self._external_only and z > 0:
      weighted_deviation_matrix = np.zeros((len(legal_actions), len(legal_actions)))
      for dev in list(info_set_node.y_values.keys()): 
        weighted_deviation_matrix += (info_set_node.y_values[dev]/z) * dev.return_transform_matrix()
      new_strategy = weighted_deviation_matrix[:,0]
      for index in range(len(legal_actions)):
        info_state_policy[legal_actions[index]] = new_strategy[index]
    
    #Full regret matching by finding the least squares solution to the fixed point 
    #Last row of matrix and the column entry ensures the solution is a strategy (otherwise would have to normalise)
    elif z > 0:
      num_actions = len(info_set_node.legal_actions)
      weighted_deviation_matrix = -np.eye(num_actions)
      
      #Calculate the 
      for dev in list(info_set_node.y_values.keys()): 
        weighted_deviation_matrix += (info_set_node.y_values[dev]/z) * dev.return_transform_matrix()

      normalisation_row = np.ones(num_actions)
      weighted_deviation_matrix = np.vstack([weighted_deviation_matrix, normalisation_row])
      b = np.zeros(num_actions+1)
      b[num_actions] = 1
      b = np.reshape(b, (num_actions+1, 1))

      strategy = lstsq(weighted_deviation_matrix, b)[0]
      normalised_strategy = strategy
      #Adopt same cutting strategy as author's code
      normalised_strategy[np.where(normalised_strategy<0)] = 0
      normalised_strategy[np.where(normalised_strategy>1)] = 1

      #Should be irrelavant
      normalised_strategy = normalised_strategy/sum(normalised_strategy)
      for index in range(len(normalised_strategy)):
        info_state_policy[info_set_node.legal_actions[index]] = normalised_strategy[index]
    #Use a uniform strategy as sum of all regrets is negative
    else:
      for index in range(len(legal_actions)):
        info_state_policy[legal_actions[index]] = 1.0 / len(legal_actions)
    
    return info_state_policy
        
def _update_average_policy(average_policy, info_state_nodes):
  """Updates in place `average_policy` to the average of all policies iterated.

  This function is a module level function to be reused by both CFRSolver and
  CFRBRSolver.

  Args:
    average_policy: A `policy.TabularPolicy` to be updated in-place.
    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.
  """
  for info_state, info_state_node in info_state_nodes.items():
    info_state_policies_sum = info_state_node.cumulative_policy
    state_policy = average_policy.policy_for_key(info_state)
    probabilities_sum = sum(info_state_policies_sum.values())
    if probabilities_sum == 0:
      num_actions = len(info_state_node.legal_actions)
      for action in info_state_node.legal_actions:
        state_policy[action] = 1 / num_actions
    else:
      for action, action_prob_sum in info_state_policies_sum.items():
        state_policy[action] = action_prob_sum / probabilities_sum
      
def strat_dict_to_array(sd):
  actions = list(sd.keys())
  strategy = np.zeros((len(actions),1))
  for action in range(len(actions)):
    strategy[action][0] = sd[actions[action]]
  return strategy

def array_to_strat_dict(sa, legal_actions):
  sd = {}
  for action in legal_actions:
    sd[action] = sa[action]
  return sd

def reset_update_var(info_nodes):
  for node in list(info_nodes.values()):
    node.updated = False
  
def create_probs_from_index(indices, current_policy):
  path_to_state = []
  if indices == None or len(indices) == 0:
    return []
  for index in indices:
    strat_dict = array_to_strat_dict(current_policy.action_probability_array[index[1]], index[0])
    path_to_state.append(strat_dict)
  return path_to_state