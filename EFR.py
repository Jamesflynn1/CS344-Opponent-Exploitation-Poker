# Copyright 2019 DeepMind Technologies Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Python implementation of the counterfactual regret minimization algorithm.

One iteration of CFR consists of:
1) Compute current strategy from regrets (e.g. using Regret Matching).
2) Compute values using the current strategy
3) Compute regrets from these values

The average policy is what converges to a Nash Equilibrium.
"""

import collections
import attr
import numpy as np
from scipy import optimize
from collections import defaultdict
from multiprocessing import Pool

from open_spiel.python import policy
#from open_spiel
import pyspiel
from open_spiel.python.algorithms.Deviation_Types.Informed_Causal import informedCausalDeviation
from open_spiel.python.algorithms.Deviation_Types.Informed_Causal import returnInformedCausalDeviation



@attr.s
class _InfoStateNode(object):
  """An object wrapping values associated to an information state."""
  # The list of the legal actions.
  legal_actions = attr.ib()
  index_in_tabular_policy = attr.ib()
  # Map from information states string representations and actions to the
  # counterfactual regrets, accumulated over the policy iterations


  # The newly availible deviations + the old ones
  relizable_deviations = attr.ib()


  cumulative_regret = attr.ib(factory=lambda: collections.defaultdict(float))

  immediate_regret = attr.ib(factory=lambda: collections.defaultdict(float))

  # Same as above for the cumulative of the policy probabilities computed
  # during the policy iterations
  cumulative_policy = attr.ib(factory=lambda: collections.defaultdict(float))
  player_history = attr.ib()


def regretProcedure(info_state, info_state_node, state_policy, iter):
      deviation_y = []
      deviations = info_state_node.relizable_deviations
      for i in range(len(deviations)):
        y = 0.0
        for h in range(len(deviations[i].timeSelectionFunctions)):
          y += max(0.00, deviations[i].timeSelectionFunctions[h](iter) *max(0, info_state_node.cumulative_regret[return_DTS_string(i,h)]))
        
        #Shouldn't happen
        if y <0:
          y= 0.0
        deviation_y.append([deviations[i], y])
      #See what regret matching retuns
      for action, value in _regret_matching(deviation_y,info_state_node.legal_actions, info_state_node).items():
        state_policy[action] = value
        info_state_node.cumulative_policy[action] = value
      return [info_state, info_state_node, state_policy]

class _EFRSolverBase(object):
  def __init__(self, game, alternating_updates, linear_averaging,
               regret_matching_plus, _deviation_gen):
    # pyformat: disable
    """Initializer.

    Args:
      game: The `pyspiel.Game` to run on.
      alternating_updates: If `True`, alternating updates are performed: for
        each player, we compute and update the cumulative regrets and policies.
        In that case, and when the policy is frozen during tree traversal, the
        cache is reset after each update for one player.
        Otherwise, the update is simultaneous.
      linear_averaging: Whether to use linear averaging, i.e.
        cumulative_policy[info_state][action] += (
          iteration_number * reach_prob * action_prob)

        or not:

        cumulative_policy[info_state][action] += reach_prob * action_prob
      regret_matching_plus: Whether to use Regret Matching+:
        cumulative_regrets = max(cumulative_regrets + regrets, 0)
        or simply regret matching:
        cumulative_regrets = cumulative_regrets + regrets
    """
    # pyformat: enable
    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, (
        "CFR requires sequential games. If you're trying to run it " +
        "on a simultaneous (or normal-form) game, please first transform it " +
        "using turn_based_simultaneous_game.")

    self._game = game
    self._num_players = game.num_players()
    self._root_node = self._game.new_initial_state()

    # This is for returning the current policy and average policy to a caller
    self._current_policy = policy.TabularPolicy(game)
    self._average_policy = self._current_policy.__copy__()
    self._deviation_gen = _deviation_gen


    self._info_state_nodes = {}
    self._initialize_info_state_nodes(self._root_node)

    self._iteration = 0  # For possible linear-averaging.
    self._linear_averaging = linear_averaging
    self._alternating_updates = alternating_updates
    self._regret_matching_plus = regret_matching_plus
  def current_policy(self):
    return self._current_policy

  def average_policy(self):
    _update_average_policy(self._average_policy, self._info_state_nodes)
    return self._average_policy

  def _initialize_info_state_nodes(self, state, history):
    """Initializes info_state_nodes.

    Create one _InfoStateNode per infoset. We could also initialize the node
    when we try to access it and it does not exist.

    Args:
      state: The current state in the tree walk. This should be the root node
        when we call this function from a CFR solver.
    """
    if state.is_terminal():
      return

    if state.is_chance_node():
      for action, unused_action_prob in state.chance_outcomes():
        self._initialize_info_state_nodes(state.child(action))
      return

    current_player = state.current_player()
    info_state = state.information_state_string(current_player)

    info_state_node = self._info_state_nodes.get(info_state)
    if info_state_node is None:
      legal_actions = state.legal_actions(current_player)
      info_state_node = _InfoStateNode(
          legal_actions=legal_actions,
          index_in_tabular_policy=self._current_policy.state_lookup[info_state],
          relizable_deviations = None,
          history = history[current_player]
          )
      info_state_node.relizable_deviations = self._deviation_gen(len(info_state_node.legal_actions), len(history[current_player]))
      self._info_state_nodes[info_state] = info_state_node
    for action in info_state_node.legal_actions:
      history[current_player].append(action)
      self._initialize_info_state_nodes(state.child(action), history.copy())

  def _update_current_policy(self, current_policy, info_state_nodes):
    """Updates in place `current_policy` from the immediate regrets
   Args:
      current_policy: A `policy.TabularPolicy` to be updated in-place.
      info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.
    """
    for info_state, info_state_node in info_state_nodes.items(): #Are these all the nodes that need updating (from start to finish)
      state_policy = current_policy.policy_for_key(info_state)
      deviation_y = []
      deviations = info_state_node.relizable_deviations
      for i in range(len(deviations)):
        y = 0.0
        y += deviations[i].player_deviation_reach_probability() *max(0, info_state_node.cumulative_regret[i])
        deviation_y.append([deviations[i], max(y,0.0)])

      #print(deviation_y)
      #See what regret matching retuns
      for action, value in _regret_matching(deviation_y,
          info_state_node.legal_actions, info_state_node).items():
        state_policy[action] = value
        info_state_node.cumulative_policy[action] += value
  def _update_current_policy2(self, current_policy, info_state_nodes):
    ifsn = []
    for info_state, info_state_node in info_state_nodes.items(): #Are these all the nodes that need updating (from start to finish)
      input_run = [info_state, info_state_node]
      input_run.append(current_policy.policy_for_key(info_state))
      input_run.append(self._iteration)
      ifsn.append(input_run)
      
    with Pool(5) as p:
        for state,node, i in p.starmap(regretProcedure, ifsn):
          p.close()
          for index in range(len(state)):
            current_policy[node.index_in_tabular_policy][index] = state[index]
            node.cumulative_policy[index] = state[index]

          cp = i
          print(cp)
          print(current_policy.policy_for_key(state))
          node.current
  #Path to state probability ignores chance probabilty as this is stored as new_reach_probabilities[-1]
  def _compute_cumulative_immediate_regret_for_player(self, state, policies,
                                                reach_probabilities, player, ):
    if state.is_terminal():
      return np.asarray(state.returns())

    if state.is_chance_node():
      state_value = 0.0
      for action, action_prob in state.chance_outcomes():
        assert action_prob > 0
        new_state = state.child(action)
        new_reach_probabilities = reach_probabilities.copy()
        new_reach_probabilities[-1] *= action_prob

        state_value += action_prob * self._compute_cumulative_immediate_regret_for_player(
            new_state, policies, new_reach_probabilities, player)
      return state_value

    current_player = state.current_player()
    info_state = state.information_state_string(current_player)

    # No need to continue on this history branch as no update will be performed
    # for any player.
    # The value we return here is not used in practice. If the conditional
    # statement is True, then the last taken action has probability 0 of
    # occurring, so the returned value is not impacting the parent node value.
    if all(reach_probabilities[:-1] == 0):
      return np.zeros(self._num_players)

    state_value = np.zeros(self._num_players)

    # The utilities of the children states are computed recursively. As the
    # regrets are added to the information state regrets for each state in that
    # information state, the recursive call can only be made once per child
    # state. Therefore, the utilities are cached.
    children_utilities = {}
    #print(state)
    #print("testing")

    info_state_node = self._info_state_nodes[info_state]
    if policies is None:
      info_state_policy = self._get_infostate_policy(info_state)
    else:
      info_state_policy = policies[current_player](info_state)
    for action in state.legal_actions():
      action_prob = info_state_policy.get(action, 0.)
      new_state = state.child(action)
      new_reach_probabilities = reach_probabilities.copy()
      new_reach_probabilities[current_player] *= action_prob
      path_to_state_probability[current_player].append(action_prob)
      child_utility = self._compute_cumulative_immediate_regret_for_player(
          new_state,
          policies=policies,
          reach_probabilities=new_reach_probabilities,
          player=player)

      state_value += action_prob * child_utility
      children_utilities[action] = child_utility

    reach_prob = reach_probabilities[current_player]
    counterfactual_reach_prob = (
        np.prod(reach_probabilities[:current_player]) *
        np.prod(reach_probabilities[current_player + 1:]))
    state_value_for_player = state_value[current_player]
    deviations = info_state_node.relizable_deviations
    for devation in range(len(deviations)):
      for timeSelection in range(len(deviations[devation].timeSelectionFunctions)):
        #info_state_node.cumulative_regret[self.return_DTS_string(devation,timeSelection)] += info_state_node.cumulative_regret[self.return_DTS_string(devation,timeSelection)]
        #for action, action_prob in info_state_policy.items():
          #current_cf_value += counterfactual_reach_prob * (children_utilities[action][current_player] - state_value_for_player)
        devation_cf_value = counterfactual_reach_prob* np.sum(np.inner(deviations[devation].deviate(info_state_node, self._get_infostate_policy(info_state)), np.array(list(children_utilities.values()))[:,current_player]))
        info_state_node.cumulative_regret[return_DTS_string(devation,timeSelection)] += deviations[devation].timeSelectionFunctions[timeSelection](self._iteration)*(devation_cf_value- counterfactual_reach_prob*state_value_for_player)
#      if self._linear_averaging:
        #info_state_node.cumulative_policy[
           # action] += self._iteration * reach_prob * action_prob
      #else:
        #info_state_node.cumulative_policy[action] += reach_prob * action_prob
    return state_value


  def _get_infostate_policy(self, info_state_str):
    """Returns an {action: prob} dictionary for the policy on `info_state`."""
    info_state_node = self._info_state_nodes[info_state_str]
    prob_vec = self._current_policy.action_probability_array[
        info_state_node.index_in_tabular_policy]
    return {
        action: prob_vec[action] for action in info_state_node.legal_actions
    }
def __get_infostate_policy_array(self, info_state_str):
    info_state_node = self._info_state_nodes[info_state_str]
    return self._current_policy.action_probability_array[
    info_state_node.index_in_tabular_policy]

def strategy_function(strategy, z, devation_regret, info_set_node):
  new_strategy = np.zeros(len(strategy))
  new_z = 0
  strategy = strategy*(strategy>=0)
  strategy_dict = {info_set_node.legal_actions[i]: strategy[i] for i in range(len(info_set_node.legal_actions))}

  assert len(strategy) == len(info_set_node.legal_actions)

  for deviationY in devation_regret: 
    new_z += deviationY[1]
    new_strategy += deviationY[1]*np.array(deviationY[0].deviate(info_set_node, strategy_dict))
  #A memebr of the n^d prob simplex
  new_strategy *= 1.0/z
  assert new_z == z
  for i in range(len(new_strategy)):
    new_strategy[i] = max (0, new_strategy[i])

  normalisation = np.sum(new_strategy)
  if normalisation != 1:
    for i in range(len(info_set_node.legal_actions)):
      new_strategy[i] = new_strategy[i] /normalisation
    return new_strategy
  else:
    return (new_strategy * 1.0)

def _regret_matching(devation_regret, legal_actions, info_set_node, external_only = True):
  """Returns an info state policy by applying regret-matching.


  Args:
    cumulative_regrets: A {deviation: y value} dictionary.
    legal_actions: the list of legal actions at this state.

  Returns:
    A dict of action -> prob for all legal actions.
  """
  z = 0.0
  #print(devation_regret)
  for dev_y in devation_regret:
    z+= dev_y[1]
  info_state_policy = {}
  #print(z)

  if external_only:
    new_strategy = []
    for deviationY in devation_regret: 
      new_z += deviationY[1]
      new_strategy += deviationY[1]*np.array(deviationY[0].deviate(info_set_node, strategy_dict))
    for i in range(len(new_strategy)):
      info_state_policy[i] = new_strategy[i]

  if z > 0 + 1e-07:
    #Use previous policy
    fallback_guess = np.full(len(legal_actions), 1.0 / len(legal_actions))
    inital_guess = np.full(len(legal_actions),1.0)

    #print (inital_guess)
    #print(info_set_node)
    try:
      new_strategy = optimize.fixed_point(strategy_function,inital_guess, args = (z, devation_regret, info_set_node), maxiter=10000, xtol = 1e-5)
      #print(new_strategy)
    except Exception as e: 
      print(e)
      new_strategy = fallback_guess
      print("failed")
    #print("NEW strat" + str(new_strategy))
    assert len(new_strategy) == len(legal_actions)
    for index in range(len(new_strategy)):
      info_state_policy[legal_actions[index]] = max(new_strategy[index],0)
  else:
    for action in legal_actions:
      info_state_policy[action] = 1.0 / len(legal_actions)
  #print(info_state_policy)
  return info_state_policy


class _EFRSolver(_EFRSolverBase):
  def __init__(self, game, alternating_updates, linear_averaging, regret_matching_plus, _deviation_gen=returnInformedCausalDeviation):
    super().__init__(game, alternating_updates, linear_averaging, regret_matching_plus, _deviation_gen)

  def evaluate_and_update_policy(self):
    """Performs a single step of policy evaluation and policy improvement."""
    self._iteration += 1
    if self._alternating_updates:
      for player in range(self._game.num_players()):
        self._compute_cumulative_immediate_regret_for_player(
            self._root_node,
            policies=None,
            reach_probabilities=np.ones(self._game.num_players() + 1),
            player=player)
        if self._regret_matching_plus:
          _apply_regret_matching_plus_reset(self._info_state_nodes)
        self._update_current_policy(self._current_policy, self._info_state_nodes)
    else:
      self._compute_cumulative_immediate_regret_for_player(
          self._root_node,
          policies=None,
          reach_probabilities=np.ones(self._game.num_players() + 1),
          player=None)
      if self._regret_matching_plus:
        _apply_regret_matching_plus_reset(self._info_state_nodes)
      self._update_current_policy(self._current_policy, self._info_state_nodes)













class EFRSolver(_EFRSolver):
  """Implements the Counterfactual Regret Minimization (CFR) algorithm.

  See https://poker.cs.ualberta.ca/publications/NIPS07-cfr.pdf

  NOTE: We use alternating updates (which was not the case in the original
  paper) because it has been proved to be far more efficient.
  """

  def __init__(self, game):
    super(EFRSolver, self).__init__(
        game,
        regret_matching_plus=False,
        alternating_updates=True,
        linear_averaging=False)


def _apply_regret_matching_plus_reset(info_state_nodes):
  """Resets negative cumulative regrets to 0.

  Regret Matching+ corresponds to the following cumulative regrets update:
  cumulative_regrets = max(cumulative_regrets + regrets, 0)

  This must be done at the level of the information set, and thus cannot be
  done during the tree traversal (which is done on histories). It is thus
  performed as an additional step.

  This function is a module level function to be reused by both CFRSolver and
  CFRBRSolver.

  Args:
    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.
  """
  for info_state_node in info_state_nodes.values():
    action_to_cum_regret = info_state_node.cumulative_regret
    for action, cumulative_regret in action_to_cum_regret.items():
      if cumulative_regret < 0:
        action_to_cum_regret[action] = 0
        
def _update_average_policy(average_policy, info_state_nodes):
  """Updates in place `average_policy` to the average of all policies iterated.

  This function is a module level function to be reused by both CFRSolver and
  CFRBRSolver.

  Args:
    average_policy: A `policy.TabularPolicy` to be updated in-place.
    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.
  """
  for info_state, info_state_node in info_state_nodes.items():
    info_state_policies_sum = info_state_node.cumulative_policy
    state_policy = average_policy.policy_for_key(info_state)
    probabilities_sum = sum(info_state_policies_sum.values())
    if probabilities_sum == 0:
      num_actions = len(info_state_node.legal_actions)
      for action in info_state_node.legal_actions:
        state_policy[action] = 1 / num_actions
    else:
      for action, action_prob_sum in info_state_policies_sum.items():
        state_policy[action] = action_prob_sum / probabilities_sum

def return_DTS_string(devation, time_selection):
    return str(devation)+" "+str(time_selection)

def return_player_history(game_history, player, number_of_players):
  for index in range(player-1,len(game_history), ):